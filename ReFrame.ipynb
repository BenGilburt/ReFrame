{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44befd8c-2c20-492f-bfe5-843642aa4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa96a44-15a7-4e81-b468-63cd6adc68de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install replicate diffusers transformers accelerate scipy\n",
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "\n",
    "!pip install torch torchvision transformers\n",
    "\n",
    "!pip install pytube opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69681bfe-37e3-42d2-baa2-47e35169a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YouTube Video Function (+ Some Dependencies)\n",
    "\n",
    "!pip install yt-dlp\n",
    "!brew install ffmpeg\n",
    "\n",
    "import yt_dlp\n",
    "\n",
    "import os\n",
    "\n",
    "# Add ffmpeg's location to PATH (update if your path differs)\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/homebrew/bin\"\n",
    "\n",
    "def download_youtube_video_single_stream(url, output_dir=\"videos\", filename=\"input_video.mp4\"):\n",
    "    \"\"\"\n",
    "    Downloads a single video stream without merging audio and video.\n",
    "    :param url: YouTube video URL\n",
    "    :param output_dir: Directory to save the video\n",
    "    :param filename: Name of the downloaded video file\n",
    "    :return: Path to the downloaded video\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    video_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"format\": \"best\",  # Download single format, no merging\n",
    "        \"outtmpl\": video_path,\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "    print(f\"Video saved to: {video_path}\")\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982bf9c-a891-4405-baa9-74574ee8165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a Video\n",
    "\n",
    "video_path = download_youtube_video_single_stream(\"https://www.youtube.com/watch?v=y8Kyi0WNg40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b243d01-37fe-4f51-bef9-e281f560fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Video into Frames Function\n",
    "\n",
    "import cv2\n",
    "\n",
    "def split_video_into_frames(video_path, frames_dir=\"frames\"):\n",
    "    \"\"\"\n",
    "    Splits a video into individual frames and saves them as images.\n",
    "    :param video_path: Path to the video file\n",
    "    :param frames_dir: Directory to save the extracted frames\n",
    "    :return: Number of frames extracted\n",
    "    \"\"\"\n",
    "    os.makedirs(frames_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Stop if the video has ended\n",
    "\n",
    "        frame_path = os.path.join(frames_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames to {frames_dir}\")\n",
    "    return frame_count\n",
    "\n",
    "frame_count = split_video_into_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5586f6-820c-492b-8994-fecb13ce6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP Interrogator Building and Folder Processing Cell\n",
    "# Takes images from the 'frames' folder and provides prompts for each, outputting a CSV file\n",
    "\n",
    "# 1. Install dependencies\n",
    "import subprocess\n",
    "\n",
    "def setup():\n",
    "    install_cmds = [\n",
    "        ['pip', 'install', 'gradio'],\n",
    "        ['pip', 'install', 'open_clip_torch'],\n",
    "        ['pip', 'install', 'clip-interrogator'],\n",
    "    ]\n",
    "    for cmd in install_cmds:\n",
    "        print(f\"Installing: {' '.join(cmd)}\")\n",
    "        subprocess.run(cmd, check=True)\n",
    "\n",
    "setup()\n",
    "\n",
    "# 2. Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "!{sys.executable} -m pip install gradio clip-interrogator\n",
    "import gradio as gr\n",
    "from clip_interrogator import Config, Interrogator\n",
    "print(\"Gradio and CLIP Interrogator installed successfully!\")\n",
    "\n",
    "# 3. Set configuration parameters\n",
    "caption_model_name = 'blip-large'  # Options: 'blip-base', 'blip-large', 'git-large-coco'\n",
    "clip_model_name = 'ViT-L-14/openai'  # Options: 'ViT-L-14/openai', 'ViT-H-14/laion2b_s32b_b79k'\n",
    "\n",
    "# 4. Initialize Interrogator\n",
    "config = Config()\n",
    "config.clip_model_name = clip_model_name\n",
    "config.caption_model_name = caption_model_name\n",
    "ci = Interrogator(config)\n",
    "\n",
    "# 5. Process all images in a folder and save prompts to CSV\n",
    "def process_images_in_folder(input_folder, output_file, mode='best'):\n",
    "    \"\"\"\n",
    "    Process all images in a folder, generate prompts, and save results to a CSV file.\n",
    "    :param input_folder: Folder containing the images\n",
    "    :param output_file: Output CSV file path\n",
    "    :param mode: Mode for prompt generation ('best', 'classic', 'fast', 'negative')\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    # Map the mode to the correct CLIP Interrogator method\n",
    "    mode_mapping = {\n",
    "        'best': ci.interrogate,\n",
    "        'classic': ci.interrogate_classic,\n",
    "        'fast': ci.interrogate_fast,\n",
    "        'negative': ci.interrogate_negative\n",
    "    }\n",
    "    \n",
    "    if mode not in mode_mapping:\n",
    "        raise ValueError(\"Invalid mode. Choose from 'best', 'classic', 'fast', or 'negative'.\")\n",
    "    \n",
    "    interrogate_method = mode_mapping[mode]\n",
    "    \n",
    "    # Iterate through all image files in the folder\n",
    "    for filename in sorted(os.listdir(input_folder)):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Only process images\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing {filename}...\")\n",
    "\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                # Use the correct interrogate method\n",
    "                prompt = interrogate_method(image)\n",
    "                prompts.append({\"filename\": filename, \"prompt\": prompt})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    if prompts:\n",
    "        df = pd.DataFrame(prompts)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved {len(prompts)} prompts to {output_file}\")\n",
    "    else:\n",
    "        print(\"No prompts were generated. Please check for errors or empty input folder.\")\n",
    "\n",
    "# 6. Process Entire Folder\n",
    "input_folder = \"frames\"  # Folder with extracted frames\n",
    "output_file = \"generated_prompts.csv\"\n",
    "\n",
    "process_images_in_folder(input_folder, output_file, mode=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1ef15-9a4c-4a48-8965-13a925c0740a",
   "metadata": {},
   "source": [
    "# I strongly recommend pausing at this stage, and re-working the prompts. The model tends to produce NSFW prompts, so it is worth working through and sanitising them prior to generating the frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e73371-16b5-445e-9eb2-10592be1ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEED TO CREATE SOMETHING THAT GENERATES THE FRAMES FROM THE PROMPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd37be-501e-40ec-9626-b1e98664a9ff",
   "metadata": {},
   "source": [
    "Note: I have temporarily managed this by running the generated prompts into Stable Diffusion running through ComfyUI. The longer-term and lower effort way would be to integrate this and feed each prompt into the model to generate the frames. Currently, prompts need to be pasted out individually. This would be a future and significant improvement to the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047b630e-88b9-4689-80ef-2600b8c06333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch the Video Back Together\n",
    "\n",
    "def frames_to_video(frames_dir=\"output_frames\", output_video=\"output_video.mp4\", fps=24):\n",
    "    \"\"\"\n",
    "    Combines frames into a video.\n",
    "    :param frames_dir: Directory containing the frames\n",
    "    :param output_video: Path to save the output video\n",
    "    :param fps: Frames per second for the video\n",
    "    \"\"\"\n",
    "    frames = sorted(glob.glob(f\"{frames_dir}/*.jpg\"))  # Ensure correct frame order\n",
    "    if not frames:\n",
    "        raise ValueError(\"No frames found in the directory!\")\n",
    "\n",
    "    # Get frame dimensions\n",
    "    first_frame = cv2.imread(frames[0])\n",
    "    height, width, _ = first_frame.shape\n",
    "\n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    for frame in frames:\n",
    "        img = cv2.imread(frame)\n",
    "        out.write(img)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823acfd-0067-4a5c-9308-6c7663d1298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below code may help in certain instances if you're having problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7aa230-ec98-41cf-b705-208b1af8da87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate Prompts from Frames 'Helper' Function\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def generate_clip_prompt(image_path):\n",
    "    \"\"\"\n",
    "    Generates a textual description for the given image using OpenAI's CLIP model.\n",
    "    :param image_path: Path to the image file\n",
    "    :return: Generated description\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "    # Normalize the image features\n",
    "    image_features = outputs / outputs.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Example text prompts (you can expand this list)\n",
    "    text_prompts = [\n",
    "        \"a photo of a cat\",\n",
    "        \"a photo of a dog\",\n",
    "        \"a scenic landscape\",\n",
    "        \"an image of a car\",\n",
    "        \"a portrait of a person\",\n",
    "        \"a surreal painting\",\n",
    "    ]\n",
    "    text_inputs = processor(text=text_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    text_features = model.get_text_features(**text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute similarities\n",
    "    similarities = torch.matmul(image_features, text_features.T)\n",
    "    best_match_idx = similarities.argmax().item()\n",
    "    return text_prompts[best_match_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681ab30-6c9e-4056-b969-1b385b40f76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame Processing Pipeline Function\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def generate_prompts_from_frames_clip(frames_dir=\"frames\", prompts_file=\"prompts_clip.txt\"):\n",
    "    \"\"\"\n",
    "    Generates prompts for each frame using CLIP.\n",
    "    :param frames_dir: Directory containing input frames\n",
    "    :param prompts_file: File to save the generated prompts\n",
    "    \"\"\"\n",
    "    frames = sorted(glob.glob(f\"{frames_dir}/*.jpg\"))  # Ensure frames are processed in order\n",
    "\n",
    "    # Ensure the output file's directory exists\n",
    "    prompts_dir = os.path.dirname(prompts_file)\n",
    "    if prompts_dir:\n",
    "        os.makedirs(prompts_dir, exist_ok=True)\n",
    "\n",
    "    with open(prompts_file, \"w\") as f:\n",
    "        for frame_path in frames:\n",
    "            print(f\"Processing frame: {frame_path}\")\n",
    "            try:\n",
    "                prompt = generate_clip_prompt(frame_path)\n",
    "                f.write(f\"{os.path.basename(frame_path)}: {prompt}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {frame_path}: {e}\")\n",
    "\n",
    "    print(f\"Prompts saved to {prompts_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab64fd7-54d5-4f3c-bbf4-4c8a8b4f6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prompts from Frames Main Function\n",
    "\n",
    "# Install and Upgrade Required Libraries\n",
    "!pip install --upgrade diffusers accelerate transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "\n",
    "# Import Libraries\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Check for Device Support (MPS for Apple Silicon or CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to Generate Prompts from Frames Using CLIP\n",
    "def generate_prompts_from_frames_clip(frames_dir=\"frames\", prompts_file=\"prompts_clip.txt\"):\n",
    "    \"\"\"\n",
    "    Generates prompts for each frame and saves them to a text file.\n",
    "    :param frames_dir: Directory containing input frames\n",
    "    :param prompts_file: File to save the generated prompts\n",
    "    \"\"\"\n",
    "    frames = sorted(glob.glob(f\"{frames_dir}/*.jpg\"))  # Ensure frames are processed in order\n",
    "\n",
    "    # Ensure the directory for the prompts file exists\n",
    "    if os.path.dirname(prompts_file):\n",
    "        os.makedirs(os.path.dirname(prompts_file), exist_ok=True)\n",
    "\n",
    "    with open(prompts_file, \"w\") as f:\n",
    "        for frame_path in frames:\n",
    "            print(f\"Processing frame: {frame_path}\")\n",
    "            try:\n",
    "                prompt = generate_clip_prompt(frame_path)  # Using the helper function\n",
    "                f.write(f\"{os.path.basename(frame_path)}: {prompt}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {frame_path}: {e}\")\n",
    "\n",
    "    print(f\"Prompts saved to {prompts_file}\")\n",
    "\n",
    "# Paths for Input Frames and Output Prompts\n",
    "frames_dir = \"frames\"\n",
    "prompts_file = \"prompts_clip.txt\"\n",
    "\n",
    "# Step: Generate Prompts from Frames\n",
    "generate_prompts_from_frames_clip(frames_dir=frames_dir, prompts_file=prompts_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
